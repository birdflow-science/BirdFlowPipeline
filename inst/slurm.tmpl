#!/bin/bash

## Job Resource Interface Definition
##
## ntasks [integer(1)]:       Number of required tasks,
##                            Set larger than 1 if you want to further parallelize
##                            with MPI within your job.
## ncpus [integer(1)]:        Number of required cpus per task,
##                            Set larger than 1 if you want to further parallelize
##                            with multicore/parallel within each task.
## walltime [integer(1)]:     Walltime for this job, in seconds.
##                            Must be at least 60 seconds for Slurm to work properly.
## memory   [integer(1)]:     Memory in megabytes for each cpu.
##                            Must be at least 100 (when I tried lower values my
##                            jobs did not start at all).
##
## Default resources can be set in your .batchtools.conf.R by defining the variable
## 'default.resources' as a named list.

<%
# relative paths are not handled well by Slurm
log.file = fs::path_expand(log.file)
-%>


#SBATCH --job-name=<%= job.name %>
#SBATCH --output=<%= log.file %>
#SBATCH --error=<%= log.file %>
#SBATCH --time=<%= resources$walltime %>
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=<%= resources$ncpus %>
#SBATCH --mem=<%= resources$memory %>G
#SBATCH --exclude=toltec-cpu[001-007]

<% 
# Define variables for gpu ram
is_gpu <- !is.null(resources$ngpus) 
# See https://docs.unity.rc.umass.edu/documentation/cluster_specs/features/
# for vram levels  e.g. "#SBATCH --constraint vram16""
vram_levels <- c(8,11,12,16,23,32,40,48,80) # 
vram <- min(vram_levels[vram_levels >= resources$memory])
has_constraint <- !is.null(resources$contraint.gpu) && !is.na(resources$constraint.gpu)
has_prefer <- !is.null(resources$prefer.gpu) && !is.na(resources$prefer.gpu)
-%>

<%= if (is_gpu) sprintf("#SBATCH --gpus=%i\n", resources$ngpus) -%>
<%= if (is_gpu && has_constraint) paste0("#SBATCH --constraint=", resources$constraint.gpu, '\n') -%>
<%= if (is_gpu && has_prefer) paste0("#SBATCH --prefer=", resources$prefer.gpu, '\n') -%>
<%= paste0("#SBATCH --partition=", if (is_gpu) resources$partition.gpu else resources$partition.cpu) %>
<%= if (array.jobs && resources$chunks.as.arrayjobs) sprintf("#SBATCH --array=1-%i", nrow(jobs)) else "" %><%= if (array.jobs && resources$chunks.as.arrayjobs) {if (!is.null(resources$ngpus)) sprintf("%%%i", resources$max.arrayjobs.gpu) else sprintf("%%%i", resources$max.arrayjobs.cpu)} else "" %>
<%= if (is_gpu) paste0("#SBATCH --constraint=vram", vram, '\n') -%>

## Export value of DEBUGME environemnt var
export DEBUGME=<%= Sys.getenv("DEBUGME") %>

## Export value of EBIRDST_KEY environemnt var
export EBIRDST_KEY=<%= Sys.getenv("EBIRDST_KEY") %>


TZ='America/Los_Angeles' date
whoami
hostname
source /etc/profile
module load apptainer/latest
apptainer exec <%= if (!is.null(resources$ngpus)) "--nv" %> /work/pi_drsheldon_umass_edu/birdflow_modeling/BirdFlowContainer45/BirdFlowContainer.sif Rscript <%= if (!is.null(resources$ngpus)) "--no-environ" %> -e 'batchtools::doJobCollection("<%= uri %>")'
TZ='America/Los_Angeles' date
